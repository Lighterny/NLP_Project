{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA5851 A3 Web Crawler: Scraping the White House News Briefings\n",
    "By Anthony Lighterness, jc244913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the White House Copyright Policy (please see https://www.whitehouse.gov/copyright/), the web-scraped press briefings for the purposes of this assignment are not copyright protected as stated: \"*Pursuant to federal law, government-produced materials appearing on this site are not copyright protected. The United States Government may receive and hold copyrights transferred to it by assignment, bequest, or otherwise*.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the web crawler presented here is to extract specific data items (date, title, style, category, and transcript) from each White House news briefing published on the official White House website (please see https://www.whitehouse.gov/news/). At the time of development, approximately 10 news briefings were available on each one of 835 pages. As such, we first extract the URL links of each one of these 6190 news briefings, which will then be used by the web scraper to extract relevant data items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "#from selenium.common.exceptions import NoSuchElementException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise ChromeDriver and Extract URLs of Each Press Briefing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise ChromeDriver\n",
    "driver_path = '/usr/local/bin/chromedriver'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "driver.get('https://www.whitehouse.gov/news/')\n",
    "driver.implicitly_wait(1) # short waiting time\n",
    "\n",
    "# Initialise empty URL list for Each News Briefing\n",
    "briefing_list = []\n",
    "\n",
    "# Please note, when subsequent press briefings are stored, the total page\n",
    "# number may change, requiring adjustment for the range(n). At the time of\n",
    "# development, 835 pages were available. A new reader can also reduce the range(n)\n",
    "# to a smaller integer like 2 or 3 to see the web crawler in action.\n",
    "\n",
    "for i in range(5):\n",
    "    # Define the URL for each briefing on every page\n",
    "    briefings = driver.find_elements_by_class_name('briefing-statement__title')\n",
    "        \n",
    "    # For each briefing URL, append to the initialised empty list.\n",
    "    for i in briefings:\n",
    "        briefing_list.append(i.find_element_by_css_selector('a').get_attribute('href'))\n",
    "        \n",
    "    # Define the next page button to be clicked to access all pages\n",
    "    element = WebDriverWait(driver,5).until(EC.element_to_be_clickable((By.CLASS_NAME,'pagination__next')))\n",
    "    driver.execute_script(\"return arguments[0].scrollIntoView();\", element)\n",
    "    # Click next page\n",
    "    element.click()\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6190"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(briefing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Extract Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for each news briefing\n",
    "def extract_data(briefing_transcripts):\n",
    "    # Initialise empty lists of variables to be extracted\n",
    "    date = []\n",
    "    category = []\n",
    "    title = []\n",
    "    transcript = []\n",
    "    style = []\n",
    "\n",
    "    # ---------- Extract Date ----------\n",
    "    try:\n",
    "        brief_date = driver.find_element_by_xpath('//*[@id=\"main-content\"]/div[1]/div/div/p/time').text\n",
    "        date.append(brief_date) \n",
    "    except:\n",
    "        date.append(\"NaN\")\n",
    "    \n",
    "    # -------- Extract Category --------\n",
    "    # Some briefings lack a specific category, so if missing, input \"NaN\"\n",
    "    try:\n",
    "        brief_category = driver.find_element_by_xpath('//*[@id=\"main-content\"]/div[1]/div/div/div/p/a').text\n",
    "        category.append(brief_category) \n",
    "    except:\n",
    "        category.append(\"NaN\")\n",
    "    \n",
    "    # ------- Extract Style/Type --------\n",
    "    # Some briefings lack a specific style/type, so if missing, input \"NaN\"\n",
    "    try:\n",
    "        brief_style = driver.find_element_by_class_name('page-header__section').text\n",
    "        style.append(brief_style)\n",
    "    except:\n",
    "        style.append(\"NaN\")\n",
    "        \n",
    "    # ---------- Extract Title ----------\n",
    "    try:\n",
    "        brief_title = driver.find_element_by_class_name('page-header__title').text\n",
    "        title.append(brief_title)\n",
    "    except:\n",
    "        title.append(\"NaN\")\n",
    "        \n",
    "    # -------- Extract Transcript -------\n",
    "    brief_transcript = driver.find_element_by_css_selector('div.page-content__content.editor').text\n",
    "    transcript.append(brief_transcript)\n",
    "\n",
    "    # Append each extracted element/variable into a table\n",
    "    briefing_transcripts.loc[len(briefing_transcripts)] = [date,title,style,category,url,transcript]\n",
    "    return(briefing_transcripts)\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White House Press Briefing Web Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22297.178542137146\n"
     ]
    }
   ],
   "source": [
    "# Measure time taken to extract text data\n",
    "start = time.time()\n",
    "\n",
    "#Â Ensure chromedriver path is set\n",
    "driver_path = '/usr/local/bin/chromedriver'\n",
    "\n",
    "# Initialise a data frame to receive extracted data\n",
    "briefing_transcripts = pd.DataFrame(columns = ['date','title','style',\n",
    "                                               'category','url','transcript'])\n",
    "\n",
    "# Crawl through each briefing URL link\n",
    "for link in briefing_list:\n",
    "    url = str(link)\n",
    "    \n",
    "    # Activate driver\n",
    "    driver = webdriver.Chrome(executable_path=driver_path) \n",
    "    \n",
    "    # Try to extract text data, otherwise quit.\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(2)\n",
    "        briefing_transcripts = extract_data(briefing_transcripts)   \n",
    "        driver.quit()        \n",
    "    except:\n",
    "        driver.quit()\n",
    "        raise\n",
    "\n",
    "# Output and save press briefing text data to an excel (xlsx) file\n",
    "briefing_transcripts.to_excel('white_house_news.xlsx', \n",
    "                 index = None, header=True)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Version info.\n",
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "# Python System Information \n",
    "print(\"Python version\")\n",
    "print (sys.version)\n",
    "print(\"Version info.\")\n",
    "print (sys.version_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=\"D:\\\\chromedriver.exe\", \n",
    "                          service_args=[\"--verbose\", \"--log-path=D:\\\\qc1.log\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
